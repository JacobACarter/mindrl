# Auto Distribution

[查看中文](./README_CN.md)

## Introduction

MindSpore Reinforcement Learning (MSRL) supports distributed training by decoupling the algorithm and execution, allowing the RL algorithm to be executed across different computing resources.
MSRL proposed an abstraction of **Fragmented Dataflow Graphs**, which strips Python functions from the RL algorithm and maps them to fragments of parallel computing. Ultimately, different fragments will be executed on different devices for data aggregation and distribution through different distributed policies (DP).
Unlike the existing distribution methods of MindSpore, the distribution of MSRL is more complex and has a larger granularity. The model parallelism and data parallelism of MS divide and parallelize the network model, while MSRL abstracts the RL algorithm into fragments such as Actor, Learner, and Environment. Based on different distribution policies, communication operators are automatically inserted to aggregate or distribute data between fragments.

<center><img src=../../docs/images/msrl_distribute.png width=600 height=350><br/>MSRL Auto Distribution Architecture</center>

## Distribution Policy

MSRL provides different distributed policies for automatic and distributed execution of single machine RL algorithms across multiple devices to improve algorithm throughput.
According to the different characteristics of the RL algorithm, the selection of distributed policies also varies. MSRL currently offers three different distributed policies.

### MultiActorSingleLearnerDP

Under this distributed policy, multiple Actors will be generated and distributed in different processes, with each Actor creating its own environment. Multiple actors will synchronously collect experience data generated by the environment, and the policy network will be collected and updated by the Learner. Meanwhile, the network parameters of the Learner will synchronously cover each Actor.

<center><img src=../../docs/images/multiactorsinglelearnerdp_detail.png width=360 height=320><br/>MultiActorSingleLearnerDP</center>

### AsyncSingleLearnerMultiActorDP

The difference between MultiActorSingleLearnerDP and AsyncSingleLearnerMultiActorDP is that multiple actors no longer synchronously collect experience and update the network. Each actor will independently and asynchronously send experience to the learner, and then the learner will apply the experience to the policy network update, covering the policy network of the actor and completing a network update.

<center><img src=../../docs/images/asyncmultiactorsinglelearnerdp_detail.png width=360 height=320><br/>AsyncSingleLearnerMultiActorDP</center>

### SingleLearnerSingleActorWithMultiEnvDP

SingleLearnerSingleActorWithMultiEnvDP binds Actor and Learner to a single process, distributing the environment across different processes. This distributed policy is suitable for situations where the environment is large or the environment node is a CPU node.

<center><img src=../../docs/images/multienvdp_detail.png width=360 height=320><br/>SingleLearnerSingleActorWithMultiEnvDP</center>

### DP & Template

Distributed Policy([DP](./distribution_policies/distribution_policy.py)) used to describe the topology logic of distributed strategies, set communication modes, set communication content, etc. Each different distributed structure has its own distributed policy. And the template(template.tp) file provides a basic skeleton for automatically generating code, which can be used in conjunction with different DPs. Basic logic can be preset in template, such as communication operator initialization, general operator definition, basic loop logic, etc. For details, please refer to [template](./distribution_policies/multi_actor_single_learner_dp/template.tp)。

### Auto Code Generation

By using a distributed policy with corresponding template, MSRL can achieve automatic distributed code generation based on existing standard single machine algorithms. MSRL will analyze the process of existing standard algorithms, use Python ast to analyze and add distributed code. Finally, according to the description of DP, the original algorithm is divided into different fragments, connected and run on different nodes through communication operators.

<center class="half">
<img src=../../docs/images/codegen.png width=800 height=300>
</center>

### Distribute Fragment

Through the above code generation logic, corresponding Fragment files will be generated in the algorithm execution path, in the form of `Fragments`+`pid`, with each process corresponding to a Fragment file.
The Fragment file generates a basic algorithm execution framework based on templates, and logically splits and communicates the original algorithm through DP description.
The following is a partial example of the `Actor` module in a Fragment, the structure of `lerner` is similar.

```python
class Actor(nn.Cell):
    def __init__(self, msrl, rank, duration, episode):
        super(Actor, self).__init__()
        self.msrl = msrl
        self.rank = rank
        self.worker_num = msrl.proc_num
        self.env_per_actor = msrl.collect_environment.num_env_per_worker
        self.allgather = P.AllGather(group=NCCL_WORLD_COMM_GROUP)
        self.broadcast = P.Broadcast(root_rank=0, group=NCCL_WORLD_COMM_GROUP)
        self.assign = P.Assign()
        self.expanddims = P.ExpandDims()
        self.depend = P.Depend()
        self.less = P.Less()

    @mindspore.jit
    def kernel(self):
        action = self.broadcast((self.action_placeholder,))[0]
        action = action[self.action_start:self.action_end]
        new_state, reward, _ = self.msrl.collect_environment.step(action)
        new_state = self.allgather(new_state)
        new_state = self.depend(new_state, action)
        reward = self.allgather(reward)
        reward = self.depend(reward, new_state)
        return reward

    def run(self):
        print('Start actor run ----- episode ', self.episode)
        for i in range(self.episode):
            res = self.gather_state()
            for j in range(self.duration):
                res = self.kernel()
            print('actor episode', i)
```

The template has three preset methods in the class `Actor`, which are`__ Init__`, used to initialize operators and pre-define variables; The `kernel` method is decorated with `jit` which means it will runs in Graph Mode to speed up. This method is the main logic of the algorithm and adds communication operators; The `run` method executes the logic for the algorithm's loop and obtains the output of the algorithm.

## Usage

Using [PPO](../../example/ppo/README_CN.md) algorithm as an example, PPO algorithm provides automatic distribution by default. In the [config.py](../algorithm/ppo/config.py) file corresponding to PPO, preset [MultiActorSingleLearnerDP](./distribution_policies/multi_actor_single_learner_dp/multi_actor_single_learner_dp.py) as default。Parameter `depoly_config` added in `config.py`, used to describe the basic information required for distribution, such as distribution strategy, number of nodes, synchronized network, etc.

```python
deploy_config = {
    "auto_distribution": True,
    "distribution_policy": MultiActorSingleLearnerDP,
    "worker_num": 2,
    "network": "actor_net",
    "algo_name": "ppo",
    "config": {},
}
```

Finally, we can execute the following command by running [train.py](../../example/ppo/train.py), achieve distributed structure under the description of `MultiActorSingleLearnerDP`. By replace the parameter in `config.py` of `distribution_policy`, it can achieve switching between different distributed strategies under the same algorithm.

> Currently, the ppo algorithm supports MultiActorSingleLearnerDP and SingleLearnerSingleActorWithMultiEnvDP, A3C supports AsyncSingleLearnerMultiActorDP, and other algorithms are under updating.

```bash
mpirun -n 4 python train.py --enable_distribute True --worker_num 4
```
